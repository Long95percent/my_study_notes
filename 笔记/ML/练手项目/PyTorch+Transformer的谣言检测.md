
https://blog.csdn.net/qq_58602552/article/details/147072281
理解完，手敲了一遍，跑通了
原代码文件名那里有点问题，稍微改一下就好
# 概念
作为初学者，可能无法理解的一些概念（我也是初学者）

## 1.向量，数组，张量
**问题场景：**
今天在问AI文本特征提取时，它告诉我输出是：
一个形状为(768,)的NumPy数组或PyTorch/TensorFlow张量，表示证据文本的提取特征向量

==Vector(向量)==  ==Array(数组，特指Numpy数组)==  ==Tensor（张量）== 它们分别是什么意思呢？

**（1）向量（Vector）:**
- **是什么**：向量是一个**数学概念**，指具有大小和方向的量。在计算机科学中，它通常指**一维数组**，即一系列有序的数字。
- **维度**：向量的“维数”就是指它包含多少个数字。
    - 例如：`[0.23, 0.45, 0.12, 0.89, ..., 0.72]` 这是一个 768 维的向量。
- **在你的场景中**：`(768,)` 这个形状描述的就是一个**768维的向量**。它代表了你的证据文本被编码后的语义信息。


**（2）数组（Array）-特指Numpy数组**
- **是什么**：NumPy 数组是 Python 科学计算库 `NumPy` 提供的一个核心数据结构。它是一个**高效的多维容器**，用于存储和处理**同类型**的数据（通常是数字）。
- **与向量的关系**：
    - **一维 NumPy 数组就是一个向量**。
    - 形状为 `(768,)` 的 NumPy 数组，就是一个 768 维的向量。
    - NumPy 数组也可以是二维的（矩阵，如 `(10, 768)` 表示10个向量）、三维的（如 `(10, 20, 768)` 一个句子序列）等等。
- **主要用途**：**通用数值计算**。它在 CPU 上运行，是 Python 数据科学生态的基础（Pandas, Scikit-learn, Matplotlib 等都构建在它之上）。


**（3）张量（Tensor）**
- **是什么**：张量是**一个更广义的数学概念**，可以看作是向量和矩阵向更高维度的推广。
    - **0维张量**：标量 (Scalar)，就是一个数字。
    - **1维张量**：向量 (Vector)。
    - **2维张量**：矩阵 (Matrix)。
    - **3维及以上张量**：高阶张量 (Higher-order Tensor)。
- **在 PyTorch/TensorFlow 中**：`Tensor` 是这两个深度学习框架中的**核心数据结构**，在概念上等同于 NumPy 数组，但有**关键增强**。
    - 形状为 `(768,)` 的 PyTorch/TensorFlow 张量，同样是一个 768 维的向量。


**（4）核心区别：NumPy 数组 vs. PyTorch/TensorFlow 张量**

虽然它们在数据存储（都是多维数组）上相似，但设计目的和功能有本质区别：

|特性|NumPy 数组 (`ndarray`)|PyTorch/TensorFlow 张量 (`Tensor`)|
|---|---|---|
|**主要用途**|**通用科学计算** (CPU)|**深度学习** (支持 GPU)|
|**硬件加速**|主要在 **CPU** 上运行|可以轻松地在 **CPU 或 GPU** 上运行，利用GPU进行大规模并行计算，极大加速训练|
|**自动求导**|**不支持**|**核心功能！** 记录计算图，可自动计算梯度（导数），这是神经网络训练（反向传播）的基石|
|**深度学习集成**|与深度学习框架是分离的|是 PyTorch/TensorFlow 的**原生公民**，与神经网络层、损失函数等无缝集成|
|**语法**|非常成熟和稳定|语法设计更适用于构建和训练动态/静态计算图|

## 2.人工智能的核心核心架构和工具链
平时看到的那些零碎概念到底是什么？

用一个建筑学的类比来串联所有概念：

1. **你想建一座新形态的摩天大楼 (Goal: 解决一个NLP问题)**
2. **你找到了一份创新的设计图纸：Transformer 架构 (The Architecture)**。这份图纸详细说明了应该使用“核心筒结构”（自注意力机制）和如何布局。
3. **你选择了一个建筑工具包：PyTorch (The Framework)**。这个工具包里包含了起重机（GPU）、混凝土搅拌机（自动求导）、标准钢材（张量操作）等所有你需要的东西。
4. **你根据图纸，使用工具包，建造了一座名为 XLM-RoBERTa 的具体大楼 (The Model)**。
5. **最后，你打开大楼的门，让用户进来做“文本特征提取” (The Application)**。